{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {
    "id": "359697d5"
   },
   "source": [
    "# LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "clN3JP9PiWrR",
   "metadata": {
    "id": "clN3JP9PiWrR"
   },
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {
    "id": "11d788b0"
   },
   "source": [
    "*Este libro de cocina se basa en la [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Goal:** Proporcionar una comprensi√≥n introductoria de los componentes y casos de uso de LangChain a trav√©s de [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) ejemplos y fragmentos de c√≥digo. Para casos de uso, consulte la parte 2 (pr√≥ximamente).\n",
    "\n",
    "\n",
    "**Links:**\n",
    "* [Documentaci√≥n conceptual de LC](https://docs.langchain.com/docs/)\n",
    "* [Documentaci√≥n de LC Python](https://python.langchain.com/en/latest/)\n",
    "* [Documentaci√≥n de LC Javascript/Mecanografiado](https://js.langchain.com/docs)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **¬øQu√© es LangChain?**\n",
    "> LangChain es un marco para desarrollar aplicaciones impulsadas por modelos de lenguaje.\n",
    "\n",
    "**~~TL~~DR**: LangChain facilita las partes complicadas de trabajar y construir con modelos de IA. Ayuda a hacer esto de dos maneras:\n",
    "\n",
    "1. **Integration** - Lleve datos externos, como sus archivos, otras aplicaciones y datos de API, a sus LLMs\n",
    "2. **Agency** - Lleve datos externos, como sus archivos, otras aplicaciones y datos de API, a sus LLMs \n",
    "\n",
    "### **Por qu√© LangChain?**\n",
    "1. **Components** - LangChain facilita el intercambio de abstracciones y componentes necesarios para trabajar con modelos de lenguaje.\n",
    "\n",
    "2. **Customized Chains** - LangChain proporciona soporte listo para usar para usar y personalizar 'cadenas', una serie de acciones unidas.\n",
    "\n",
    "3. **Speed üö¢** - Este equipo se env√≠a incre√≠blemente r√°pido. Estar√°s al d√≠a con las √∫ltimas funciones de LLMs.\n",
    "\n",
    "4. **Community üë•** - Maravilloso discord y apoyo comunitario, encuentros, hackatones, etc.\n",
    "\n",
    "Aunque los LLMs pueden ser sencillos (entrada de texto, salida de texto), r√°pidamente se encontrar√° con puntos de fricci√≥n con los que LangChain ayuda una vez que desarrolla aplicaciones m√°s complicadas.\n",
    "\n",
    "*Nota: este libro de cocina no cubrir√° todos los aspectos de LangChain. Su contenido ha sido curado para que puedas construir e impactar lo m√°s r√°pido posible. Para obtener m√°s informaci√≥n, consulte [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9815081",
   "metadata": {
    "hide_input": false,
    "id": "e9815081"
   },
   "outputs": [],
   "source": [
    "openai_api_key='openai_api_key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {
    "id": "05bb564d"
   },
   "source": [
    "# LangChain Components\n",
    "\n",
    "## Esquema: tuercas y tornillos de trabajar con LLM\n",
    "\n",
    "### **Text**\n",
    "La forma de lenguaje natural para interactuar con los LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false,
    "id": "8e0dc06c"
   },
   "outputs": [],
   "source": [
    "# Trabajar√° con cadenas simples (¬°que pronto crecer√°n en complejidad!)\n",
    "my_text = \"What day comes after Friday?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {
    "id": "2f39eb39"
   },
   "source": [
    "### **Chat Messages**\n",
    "Como texto, pero especificado con un tipo de mensaje (Sistema, Humano, AI)\n",
    "\n",
    "* **Sistema**: contexto de fondo √∫til que le dice a la IA qu√© hacer\n",
    "* **Humano**: mensajes destinados a representar al usuario\n",
    "* **IA** - Mensajes que muestran con qu√© respondi√≥ la IA\n",
    "\n",
    "Para obtener m√°s informaci√≥n, consulte OpenAI [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "xu8gww5iisBS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xu8gww5iisBS",
    "outputId": "74259e7e-e635-4dda-9fa2-6b176c3d56cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.141)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.1.11)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (0.27.4)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99b0935b",
   "metadata": {
    "id": "99b0935b"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "878d6a36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "878d6a36",
    "outputId": "e7d2e130-aecf-4db2-9216-6cb64224e91b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You can make a delicious Caprese salad with tomatoes, fresh mozzarella, and basil.', additional_kwargs={})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        # Contexto del dialogo\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        # Dialogo humano \n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {
    "id": "0a425aaa"
   },
   "source": [
    "Tambi√©n puede pasar m√°s historial de chat con respuestas de la IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fd3fe88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fd3fe88",
    "outputId": "a57da97e-d1ba-4720-d17d-e4399c864cee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"While you're in Nice, you could also take a stroll along the famous Promenade des Anglais and explore the charming Old Town.\", additional_kwargs={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        # Contexto del dialogo\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        # Dialogo humano\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        # Dialogo maquina\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        # Dialogo humano\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {
    "id": "66bf9634"
   },
   "source": [
    "### **Documentos**\n",
    "Un objeto que contiene un fragmento de texto y metadatos (m√°s informaci√≥n sobre ese texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bbf58b2",
   "metadata": {
    "id": "3bbf58b2"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "150e8759",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "150e8759",
    "outputId": "06749a50-c083-438a-ea9e-89682ea1dc84",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PyE89Pt4UPr",
   "metadata": {
    "id": "7PyE89Pt4UPr"
   },
   "source": [
    "Esto es √∫til cuando tienes gran cantidad de documentos en tu base de datos y necesitas filtarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {
    "id": "2e462b5d"
   },
   "source": [
    "## Modelos: la interfaz para los cerebros de IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe982",
   "metadata": {
    "id": "b27fe982"
   },
   "source": [
    "### **Modelo de lenguaje**\n",
    "¬°Un modelo que hace texto adentro ‚û°Ô∏è texto afuera!\n",
    "\n",
    "*Mira c√≥mo cambi√© el modelo que estaba usando del predeterminado a ada-001. Ver m√°s modelos [aqu√≠](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74b1a72a",
   "metadata": {
    "id": "74b1a72a"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6399c295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6399c295",
    "outputId": "fcb2f3ba-2da4-44aa-a962-fb3e6ceb9093"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nSaturday'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {
    "id": "3ef89bfa"
   },
   "source": [
    "### **Modelo de chat**\n",
    "Un modelo que toma una serie de mensajes y devuelve una salida de mensaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf091777",
   "metadata": {
    "id": "bf091777"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4260711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4260711",
    "outputId": "c4f822fb-2adc-4f9e-8031-ba52a118462d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You should probably start by walking, and if that takes too long, try hopping on a pogo stick. They're surprisingly efficient.\", additional_kwargs={})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {
    "id": "c2b70f23"
   },
   "source": [
    "### **Modelo de incrustaci√≥n de texto**\n",
    "Cambie su texto a un vector (una serie de n√∫meros que contienen el 'significado' sem√°ntico de su texto). Se utiliza principalmente cuando se comparan dos fragmentos de texto.\n",
    "\n",
    "*Por cierto: sem√°ntico significa 'relacionado con el significado en el lenguaje o la l√≥gica'.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1655de82",
   "metadata": {
    "id": "1655de82"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2c85e7e",
   "metadata": {
    "id": "a2c85e7e"
   },
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "hbaxdpMCjoiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbaxdpMCjoiN",
    "outputId": "ac21cfca-7e66-4adb-e31d-67e386ed2ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddc5a368",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddc5a368",
    "outputId": "fed1d594-22cd-47c6-fd71-42d4340b800d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [-0.00015641732627415968, -0.003165106234320816, -0.0008140143913923668, -0.019451457670515905, -0.015182807778854484]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {
    "id": "c38fe99f"
   },
   "source": [
    "## Indicaciones: texto generalmente utilizado como instrucciones para su model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {
    "id": "8b9318ed"
   },
   "source": [
    "### **Inmediato**\n",
    "Lo que pasar√° al modelo subyacente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d270239",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2d270239",
    "outputId": "c6c17faf-ac97-4f4a-a5f7-1db72e463d1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'La afirmaci√≥n es incorrecta. Ma√±ana es martes.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Me gusta usar tres comillas dobles para mis indicaciones porque es m√°s f√°cil de leer.\n",
    "prompt = \"\"\"\n",
    "Hoy es lunes, ma√±ana es mi√©rcoles.\n",
    "\n",
    "¬øQu√© hay de malo en esa afirmaci√≥n??\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {
    "id": "74988254"
   },
   "source": [
    "### **Prompt Template**\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abcc212d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abcc212d",
    "outputId": "dcb1d9c0-c623-4b5f-8329-e0aa3dec9fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "Tengo muchas ganas de viajar a Rome. ¬øQu√© debo hacer ah√≠?\n",
      "\n",
      "Responde en una oraci√≥n corta\n",
      "\n",
      "-----------\n",
      "LLM Output: \n",
      "Explorar los monumentos y la cultura de Roma.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "Tengo muchas ganas de viajar a {location}. ¬øQu√© debo hacer ah√≠?\n",
    "\n",
    "Responde en una oraci√≥n corta\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {
    "id": "ed40bac2"
   },
   "source": [
    "### **Selectores de ejemplo**\n",
    "Una manera f√°cil de seleccionar entre una serie de ejemplos que le permiten colocar din√°micamente informaci√≥n en contexto en su indicaci√≥n. A menudo se usa cuando su tarea tiene matices o tiene una gran lista de ejemplos.\n",
    "\n",
    "consulte los diferentes tipos de selectores de ejemplo [aqu√≠](https://python.langchain.com/en/latest/modules/prompts/example_selectors.html)\n",
    "\n",
    "Si desea obtener una descripci√≥n general de por qu√© los ejemplos son importantes (ingenier√≠a r√°pida), consulte [este video](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaf36cd9",
   "metadata": {
    "id": "aaf36cd9"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Ejemplos de lugares donde se encuentran los sustantivos\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12b4798b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12b4798b",
    "outputId": "f9920771-e78c-4d59-cfd2-0b34f0ea3859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "# SemanticSimilarityExampleSelector seleccionar√° ejemplos que son similares a su entrada por significado sem√°ntico\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Esta es la lista de ejemplos disponibles para seleccionar.\n",
    "    examples, \n",
    "    \n",
    "    # Esta es la clase de incrustaci√≥n utilizada para producir incrustaciones que se utilizan para medir la similitud sem√°ntica.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # Esta es la clase VectorStore que se usa para almacenar las incrustaciones y realizar una b√∫squeda de similitud.\n",
    "    FAISS, \n",
    "    \n",
    "    # Este es el n√∫mero de ejemplos a producir.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cf30107",
   "metadata": {
    "id": "2cf30107"
   },
   "outputs": [],
   "source": [
    "\n",
    "# \"FewShotPromptTemplate\" significa que le daremos pocos ejemplos \n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # El objeto que ayudar√° a seleccionar ejemplos.\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Tu prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Personalizaciones que se agregar√°n en la parte superior e inferior de su \"prompt\"\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # Qu√© entradas recibir√° su aviso\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "369442bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "369442bb",
    "outputId": "893fe925-2bd9-4fc6-e51c-7f9bd9ea89b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: pilot\n",
      "Example Output: plane\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Seleccione un sustantivo!\n",
    "my_noun = \"student\"\n",
    "\n",
    "# Buscamos los prompt similares\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bb910f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9bb910f2",
    "outputId": "9e91343a-e2b8-44f6-a915-05ae5dc07d4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' classroom'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {
    "id": "8474c91d"
   },
   "source": [
    "### **Output Parsers**\n",
    "Una forma √∫til de dar formato a la salida de un modelo. Usualmente se usa para salida estructurada.\n",
    "\n",
    "Dos grandes conceptos:\n",
    "\n",
    "**1. Instrucciones de formato**: un aviso generado autom√°ticamente que le dice al LLM c√≥mo formatear su respuesta en funci√≥n del resultado deseado\n",
    "\n",
    "**2. Analizador**: un m√©todo que extraer√° la salida de texto de su modelo en una estructura deseada (generalmente json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58353756",
   "metadata": {
    "id": "58353756"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee36f881",
   "metadata": {
    "id": "ee36f881"
   },
   "outputs": [],
   "source": [
    "# Intaciamos nuestros LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa59be3f",
   "metadata": {
    "id": "fa59be3f"
   },
   "outputs": [],
   "source": [
    "# C√≥mo le gustar√≠a que se estructurara su respuesta. Esta es b√°sicamente una plantilla de aviso elegante\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# C√≥mo le gustar√≠a analizar su salida\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1079f0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1079f0a",
    "outputId": "bc0f8d7b-7806-49eb-820a-0ccec0257bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Ver la plantilla de solicitud que cre√≥ para formatear\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9aaae5be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aaae5be",
    "outputId": "f957eb03-77cd-4fe2-d2f7-c1628258dab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se le dar√° una cadena mal formateada de un usuario.\n",
      "Vuelva a formatearlo y aseg√∫rese de que todas las palabras est√©n escritas correctamente\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Se le dar√° una cadena mal formateada de un usuario.\n",
    "Vuelva a formatearlo y aseg√∫rese de que todas las palabras est√©n escritas correctamente\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b116bb23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b116bb23",
    "outputId": "0eaacb49-892a-4a7c-c2ed-ba6574ed36df"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "985aa814",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "985aa814",
    "outputId": "d733a7b5-580b-45d2-eef9-53ef2cdcd777"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {
    "id": "7b43cec2"
   },
   "source": [
    "## √çndices: la estructuraci√≥n de documentos para LLM puede funcionar con ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {
    "id": "d3f904e9"
   },
   "source": [
    "### **Document Loaders**\n",
    "Maneras f√°ciles de importar datos de otras fuentes. Funcionalidad compartida con [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins)[complementos espec√≠ficamente de recuperaci√≥n](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "Ver una [lista grande](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)de cargadores de documentos aqu√≠. Mucho m√°s en [Llama Index](https://llamahub.ai/) tambi√©n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba88e05b",
   "metadata": {
    "id": "ba88e05b"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee693520",
   "metadata": {
    "id": "ee693520"
   },
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88d89ad7",
   "metadata": {
    "id": "88d89ad7"
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e814f930",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e814f930",
    "outputId": "4294d413-af26-4a41-b3b2-ddf7720d26a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "Here's a sample:\n",
      "\n",
      "Ozzie_osman 88 days ago  \n",
      "             | next [‚Äì] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very pOzzie_osman 88 days ago  \n",
      "             | parent | next [‚Äì] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index) \n"
     ]
    }
   ],
   "source": [
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {
    "id": "0e9601db"
   },
   "source": [
    "### **Divisores de texto**\n",
    "Muchas veces su documento es demasiado largo (como un libro) para su LLM. Necesitas dividirlo en pedazos. Los divisores de texto ayudan con esto.\n",
    "\n",
    "Hay muchas formas de dividir el texto en fragmentos, experimentar con [diferentes](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) para ver cu√°l es mejor para usted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95713e57",
   "metadata": {
    "id": "95713e57"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a54455f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a54455f5",
    "outputId": "90beffc5-0eea-47d8-e1b1-ea26b1125e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n"
     ]
    }
   ],
   "source": [
    "# Este es un documento largo que podemos dividir.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d19acb18",
   "metadata": {
    "id": "d19acb18"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Establezca un tama√±o de fragmento realmente peque√±o, solo para mostrar.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3090f05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3090f05",
    "outputId": "52b57502-f40a-4532-8c58-73125b60ae78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 606 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87a0f45a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87a0f45a",
    "outputId": "729565c0-6b21-4f23-b8c4-8db953a78da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {
    "id": "1f85defb"
   },
   "source": [
    "### **Retrievers**\n",
    "Manera f√°cil de combinar documentos con modelos de lenguaje.\n",
    "\n",
    "Hay muchos tipos diferentes de Retrievers, el m√°s ampliamente compatible es el VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8cccbd82",
   "metadata": {
    "id": "8cccbd82"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1dab1c20",
   "metadata": {
    "id": "1dab1c20"
   },
   "outputs": [],
   "source": [
    "# Prepare su divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Divide tus documentos en textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preparar el motor de incrustaci√≥n\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Incrusta tus textos\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e62372be",
   "metadata": {
    "id": "e62372be"
   },
   "outputs": [],
   "source": [
    "# Inicie su Retrievers. Pedir solo 1 documento de vuelta\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0534bbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0534bbd",
    "outputId": "aa7f3c61-59bd-44cf-ccb7-ab22719ab33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f124cacc970>, search_type='similarity', search_kwargs={})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3846a3b5",
   "metadata": {
    "id": "3846a3b5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Internamente lo que hace esto es convertir nuestra petici√≥n en una verctor \n",
    "y conparalo con los vectores de cada fracmento de nuestro texto buscando cual tienen m√°s similitud\n",
    "\"\"\"\n",
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db383cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db383cc8",
    "outputId": "bc291c54-f94f-4635-8156-dbbd102972d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "been generating for galleries. This impressive-sounding thing called\n",
      "an \"internet storefront\" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready co\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {
    "id": "24193139"
   },
   "source": [
    "### **VectorStores**\n",
    "Bases de datos para almacenar vectores. Los m√°s populares son [Pinecone](https://www.pinecone.io/) y [Weaviate](https://weaviate.io/). M√°s ejemplos en OpenAIs [documentaci√≥n del recuperador](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) son f√°ciles de trabajar localmente.\n",
    "\n",
    "Conceptualmente, piense en ellos como tablas con una columna para incrustaciones (vectores) y una columna para metadatos..\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3c5533ad",
   "metadata": {
    "id": "3c5533ad"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Prepara tu divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Divide tus documentos en textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preparar el motor de incrustaci√≥n\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "661fdf19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "661fdf19",
    "outputId": "2705f868-a7d7-4a76-952b-abc9229f5ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e99ac0ea",
   "metadata": {
    "id": "e99ac0ea"
   },
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89e7758c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89e7758c",
    "outputId": "9b000ed9-b2bf-4fdc-f346-d39190a781ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 embeddings\n",
      "Here's a sample of one: [-0.0031578343974066592, -0.011530040306841302, -0.013900515073065738]...\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac358c5",
   "metadata": {
    "id": "8ac358c5"
   },
   "source": [
    "Su tienda de vectores almacena sus incrustaciones (‚òùÔ∏è) y hace que sea f√°cil de buscar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {
    "id": "f9b9b79b"
   },
   "source": [
    "## Memory\n",
    "Ayudar a los LLMs a recordar informaci√≥n.\n",
    "\n",
    "La memoria es un t√©rmino un poco vago. Puede ser tan simple como recordar informaci√≥n sobre la que conversaron en el pasado o una recuperaci√≥n de informaci√≥n m√°s complicada.\n",
    "\n",
    "Lo mantendremos en el caso de uso de mensajes de chat. Esto se usar√≠a para los bots de chat.\n",
    "\n",
    "Hay muchos tipos de memoria, explore [la documentaci√≥n](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) para ver cu√°l\n",
    "\n",
    "Traducir esta p√°gina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {
    "id": "f43b49da"
   },
   "source": [
    "### Historial de mensajes de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "893a18c1",
   "metadata": {
    "id": "893a18c1"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2949fda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2949fda",
    "outputId": "e34c03c3-7e9a-4ae8-fc07-19f9fa273af5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={})]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b74d5cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b74d5cf",
    "outputId": "4373ca65-1f25-4ddf-cfed-171744b23b0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "529e168f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "529e168f",
    "outputId": "3bf491b5-4d66-4d49-93bb-c7560c1f3e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={})]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {
    "id": "f29fc79c"
   },
   "source": [
    "## Chains ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "combinar diferentes llamadas y acciones del LLM autom√°ticamente\n",
    "\n",
    "Ejemplo: Resumen n.¬∞ 1, Resumen n.¬∞ 2, Resumen n.¬∞ 3 > Resumen final\n",
    "\n",
    "Mire [este video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) que explica diferentes tipos de cadenas de resumen\n",
    "\n",
    "Hay [muchas aplicaciones de cadenas](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) busque para ver cu√°les son las mejores para su caso de uso.\n",
    "\n",
    "Cubriremos dos de ellos:\n",
    "\n",
    "Traducir esta p√°gina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {
    "id": "c34ba415"
   },
   "source": [
    "### 1. Cadenas secuenciales simples\n",
    "\n",
    "Cadenas f√°ciles donde puede usar la salida de un LMM como entrada en otro. Bueno para dividir tareas (y mantener tu LLM enfocado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79fc0950",
   "metadata": {
    "id": "79fc0950"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "43d4494a",
   "metadata": {
    "id": "43d4494a"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "# prompt de ubicaci√≥n\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Contiene mi cadena de 'ubicaci√≥n'\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b6c8e00f",
   "metadata": {
    "id": "b6c8e00f"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "# prompt de comida\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Contiene mi cadena de 'comida'\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e0b83f2",
   "metadata": {
    "id": "7e0b83f2"
   },
   "outputs": [],
   "source": [
    "# Unimos las dos cadenes un pasos secuenciales.\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7d19c64d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d19c64d",
    "outputId": "2ee2a14d-9e73-4c3f-a214-9c8fe26a893f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mOne classic dish from Rome is Cacio e Pepe, a pasta dish made with Pecorino Romano cheese and black pepper.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ingredients:\n",
      "\n",
      "    12 ounces spaghetti \n",
      "    4 tablespoons butter \n",
      "    1 teaspoon freshly cracked black pepper \n",
      "    2/3 cup grated Pecorino-Romano cheese\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a pot of salted water to a boil. Cook the spaghetti pasta al dente (around 8 minutes). \n",
      "\n",
      "2. In a large skillet, melt the butter over low heat. \n",
      "\n",
      "3. When the pasta is cooked, strain and add to the skillet, along with the pepper and Pecorino-Romano cheese. \n",
      "\n",
      "4. Toss the pasta with tongs to ensure it is coated with the butter and cheese. \n",
      "\n",
      "5. Serve immediately and enjoy!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Vemos su ejecucion\n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {
    "id": "f6191bf5"
   },
   "source": [
    "### 2. Cadena de resumen\n",
    "\n",
    "Ejecute f√°cilmente numerosos documentos largos y obtenga un resumen. Mire [este video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) para ver otros tipos de cadenas adem√°s de map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6f218c3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6f218c3e",
    "outputId": "a7d4e950-06f2-4630-d65e-91e087eab038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\n",
      "Biographies of famous scientists often omit their mistakes, leading to a skewed view of the risks taken by these figures. Smart people often make risky decisions outside of the realm of success, as is exemplified by Newton's dedication to alchemy and theology. It is possible, however, that these choices were not a mistake but simply a different way to explore their discoveries.\n",
      "\n",
      " In his day, Newton gambled on three risky bets, one of which ended up being the invention of what we now call physics. At the time, alchemy and theology were also seen as potentially rewarding prospects, showing that the separation of smartness and craziness isn't as clear as we think it is.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" The biographies of famous scientists often don't mention their risky and unsuccessful decisions, but Isaac Newton's decisions to pursue alchemy and theology prove that smart choices don't always result in success. His exploration of these topics may have ultimately led to the invention of what we now call physics. Smartness and craziness can often be intertwined.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Prepare su divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Divide tus documentos en textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Esta linea resumenes de los fracmentos de texto de cada divici√≥n y luego hara un resumen general usando los anteriroes(Esto bajo la estructura de una cadena)\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {
    "id": "84f6193c"
   },
   "source": [
    "## Agents ü§ñü§ñ\n",
    "\n",
    "La documentaci√≥n oficial de LangChain describe a los agentes perfectamente (√©nfasis m√≠o):\n",
    "> Algunas aplicaciones requerir√°n no solo una cadena predeterminada de llamadas a LLM/otras herramientas, sino potencialmente una **cadena desconocida** que depende de la entrada del usuario. En este tipo de cadenas, hay un ‚Äúagente‚Äù que tiene acceso a un conjunto de herramientas. Dependiendo de la entrada del usuario, el agente puede entonces **decidir a cu√°l de estas herramientas, si alguna, llamar**.\n",
    "\n",
    "\n",
    "B√°sicamente, utiliza el LLM no solo para la salida de texto, sino tambi√©n para la toma de decisiones. La genialidad y el poder de esta funcionalidad no se pueden exagerar lo suficiente.\n",
    "\n",
    "Sam Altman enfatiza que los LLM son buenos '[motor de razonamiento](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agente aproveche esto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {
    "id": "3ce05d51"
   },
   "source": [
    "### Agentes\n",
    "\n",
    "El modelo de lenguaje que impulsa la toma de decisiones.\n",
    "\n",
    "M√°s espec√≠ficamente, un agente toma una entrada y devuelve una respuesta correspondiente a una acci√≥n a realizar junto con una entrada de acci√≥n. Puede ver diferentes tipos de agentes (que son mejores para diferentes casos de uso) [aqu√≠](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {
    "id": "f696b65c"
   },
   "source": [
    "### Tool/Herramientas\n",
    "\n",
    "Una 'capacidad' de un agente. Esta es una abstracci√≥n sobre una funci√≥n que facilita que los LLM (y los agentes) interact√∫en con ella. Ej: b√∫squeda en Google.\n",
    "\n",
    "Esta √°rea comparte puntos en com√∫n con los [complementos de OpenAI] (https://platform.openai.com/docs/plugins/introduction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {
    "id": "a11f8231"
   },
   "source": [
    "### Caja de herramientas/Toolkit\n",
    "\n",
    "Grupos de herramientas que su agente puede seleccionar\n",
    "\n",
    "Reun√°moslos todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67d5d82d",
   "metadata": {
    "id": "67d5d82d"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ddcdbb9",
   "metadata": {
    "id": "0ddcdbb9"
   },
   "outputs": [],
   "source": [
    "serpapi_api_key='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "44fad67f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44fad67f",
    "outputId": "83d8f691-60f5-49af-e3fb-4fa43ebaa771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from google-search-results) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (1.26.15)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32019 sha256=c49fb0c0b6e21e278f912adaf531693655ae47f2954b86105c3f21e3dd958e36\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/8e/73/744b7d9d7ac618849d93081a20e1c0deccd2aef90901c9f5a9\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results\n",
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=\"Tu API de serpapi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f544a74b",
   "metadata": {
    "id": "f544a74b"
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c4882754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4882754",
    "outputId": "fb69b3af-d959-44c1-f0c6-9913a525e74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try to find out what band Natalie Bergman is a part of.\n",
      "Action: Search\n",
      "Action Input: \"Natalie Bergman band\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNatalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for the debut album of Wild Belle.\n",
      "Action: Search\n",
      "Action Input: \"Wild Belle debut album\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Isles is the debut album of Wild Belle, the band that Natalie Bergman is a part of.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"what was the first album of the\" \n",
    "                    \"band that Natalie Bergman is a part of?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ba438064",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba438064",
    "outputId": "99a588d2-c706-4392-942b-6f52b2cc2f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    [\n",
      "      \"Search\",\n",
      "      \"Natalie Bergman band\",\n",
      "      \" I should try to find out what band Natalie Bergman is a part of.\\nAction: Search\\nAction Input: \\\"Natalie Bergman band\\\"\"\n",
      "    ],\n",
      "    \"Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.\"\n",
      "  ],\n",
      "  [\n",
      "    [\n",
      "      \"Search\",\n",
      "      \"Wild Belle debut album\",\n",
      "      \" I should search for the debut album of Wild Belle.\\nAction: Search\\nAction Input: \\\"Wild Belle debut album\\\"\"\n",
      "    ],\n",
      "    \"Isles\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(response[\"intermediate_steps\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c30d2",
   "metadata": {
    "id": "3f9c30d2"
   },
   "source": [
    "![Wild Belle](data/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b368",
   "metadata": {
    "id": "14f4b368"
   },
   "source": [
    "üéµDisfrutaüéµhttps://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
