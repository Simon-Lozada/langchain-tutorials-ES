{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ed4da4d",
      "metadata": {
        "id": "2ed4da4d"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Esto tiene formato de código\n",
        "```\n",
        "\n",
        "## Guía de inicio rápido\n",
        "https://langchain.readthedocs.io/en/latest/getting_started/getting_started.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a05530b1",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a05530b1",
        "outputId": "ca512163-56d6-4dc1-98da-669e20163070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.140-py3-none-any.whl (539 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m539.5/539.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Collecting SQLAlchemy<2,>=1\n",
            "  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gptcache>=0.1.7\n",
            "  Downloading gptcache-0.1.11-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n",
            "Installing collected packages: SQLAlchemy, mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, openai, gptcache, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.9\n",
            "    Uninstalling SQLAlchemy-2.0.9:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.9\n",
            "Successfully installed SQLAlchemy-1.4.47 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 gptcache-0.1.11 langchain-0.0.140 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.4 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "# Instalamos Lang Chain\n",
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3b556fc2",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b556fc2",
        "outputId": "40e05861-af27-4900-9817-680b7ee5cbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n"
          ]
        }
      ],
      "source": [
        "# Instalamos openai parausar un modelo gpt como base de nuestras pruevas\n",
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0c07fe20",
      "metadata": {
        "id": "0c07fe20"
      },
      "outputs": [],
      "source": [
        "# Establecemos nuestras API KEY de openAI\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Ponga su API key aqui\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0126a6a1",
      "metadata": {
        "id": "0126a6a1"
      },
      "source": [
        "# Creación de una aplicación de modelo de lenguaje\n",
        "### LLMS: Obtener predicciones de un modelo de lenguaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4bef95a9",
      "metadata": {
        "id": "4bef95a9"
      },
      "outputs": [],
      "source": [
        "# Importamos el interprete del LLM de OpenAI\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3596e426",
      "metadata": {
        "id": "3596e426"
      },
      "outputs": [],
      "source": [
        "# cargamos el modelo\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9ebe6d15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ebe6d15",
        "outputId": "5bff5e50-2719-46e2-a42e-48b298ad2ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Rome, Italy\n",
            "2. Naples, Italy\n",
            "3. Naples, Florida\n",
            "4. Bologna, Italy\n",
            "5. Venice, Italy\n"
          ]
        }
      ],
      "source": [
        "# Prueva simple\n",
        "\n",
        "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7657e1e9",
      "metadata": {
        "id": "7657e1e9"
      },
      "source": [
        "### Prompt Templates: administre solicitudes para LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "168f2277",
      "metadata": {
        "id": "168f2277"
      },
      "outputs": [],
      "source": [
        "# Cargamos la herramienta para crear nuestras plantillas\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d6d3761",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "9d6d3761",
        "outputId": "e87e2a8c-6ce9-4440-e85b-ec0451f5a5fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4bdb8bf9a131>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m prompt = PromptTemplate(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ingridientes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"especias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tengo los siguientes ingridientes {ingridientes} y las siguientes especias {especias} \\n Dime la preparación de 3 recetas sencillas paso a paso\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\" \n",
        "Creamos la plantilla. Este tiene\n",
        "  *template = plantilla. Es la estrctura de la pregunta, esta puede tener:\n",
        "    - Parametros de respuesta\n",
        "    - Tono de la respuesta\n",
        "    - Contexto\n",
        "  *input_variables = Variables. Son varibles dentro de nuestra plantilla estas hacen nuestra plantilla reusable\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo de PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"ingridientes\", \"especias\"],\n",
        "    template=\"Tengo los siguientes ingridientes {ingridientes} y las siguientes especias {especias} \\n Dime la preparación de 3 recetas sencillas paso a paso\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "160f8a0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "160f8a0f",
        "outputId": "0b757bd3-fb24-45fd-dfc4-1d2042e9db79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are 5 vacation destinations for someone who likes to eat dessert whit water?\n"
          ]
        }
      ],
      "source": [
        "print(prompt.format(food=\"Carne, Pescado, Queso, Pasta, tomate, cebolla, Pimientos\", drink=\"Sal, pimienta\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4309f2a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4309f2a0",
        "outputId": "be75ee45-dce1-4e16-8874-2cde9bcd8787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Bali, Indonesia - Known for its tropical climate and stunning beaches, Bali is a paradise for those who love to indulge in exotic desserts, refreshing water, and sun-soaked relaxation.\n",
            "\n",
            "2. Hawaii - From white-sand beaches and fertile valleys to lush jungles and cascading waterfalls, Hawaii is a postcard-perfect destination for those looking to enjoy some of the best desserts and waters in the world.\n",
            "\n",
            "3. Tokyo, Japan - Home to an incredible range of desserts, from traditional Japanese sweets to modern Western-style creations, Tokyo is a perfect destination for those who can’t get enough of sweet treats.\n",
            "\n",
            "4. French Riviera - It's impossible to spend your days in the French Riviera without indulging in some of the best desserts France has to offer. Enjoy cool waters on the beach and top it all off with a creamy tart or a warm apple tart.\n",
            "\n",
            "5. Istanbul, Turkey - With its ancient culture and stunning architecture, Istanbul is a great destination for travelers who want to explore the city and treat themselves to a sweet delight. Head to the city's many patisseries to sample some traditional goodies made with water.\n"
          ]
        }
      ],
      "source": [
        "print(llm(prompt.format(food=\"dessert\", drink=\"water\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd0c523",
      "metadata": {
        "id": "9cd0c523"
      },
      "source": [
        "### C[texto del vínculo](https://)hain: combine LLMs y prompts en multiple plazos de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a34ff024",
      "metadata": {
        "id": "a34ff024"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ffbdf4a9",
      "metadata": {
        "id": "ffbdf4a9"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7efda77b",
      "metadata": {
        "id": "7efda77b"
      },
      "outputs": [],
      "source": [
        "# Juntamos nuestra plantilla con nuestro modelo\n",
        "# (Parecido a un piline)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0b08b76a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b08b76a",
        "outputId": "d29cd514-2305-4073-ad95-20edf368840e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Hawaii - With its abundance of tropical fruits, Hawaii is a great destination for fruit lovers.\n",
            "2. Costa Rica - Costa Rica is known for its delicious tropical fruits, such as mangoes, papayas, and pineapples.\n",
            "3. Thailand - Thailand is home to a variety of exotic fruits, such as durian, mangosteen, and rambutan.\n",
            "4. India - India is a great destination for fruit lovers, with its wide variety of fruits, such as lychees, jackfruit, and guavas.\n",
            "5. South Africa - South Africa is home to a variety of delicious fruits, such as mangoes, bananas, and oranges.\n"
          ]
        }
      ],
      "source": [
        "# Resultado\n",
        "print(chain.run(\"fruit\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "850668f0",
      "metadata": {
        "id": "850668f0"
      },
      "source": [
        "### Agents: Cadenas de llamadas dinámicas basadas en la entrada del usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c19db4aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c19db4aa",
        "outputId": "710d6896-e903-4da1-d540-b56e361b8b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->google-search-results) (3.4)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32019 sha256=ebe971cded02143f125172fb63357e805b24c668c19b50a50b5d4561a3f825fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/8e/73/744b7d9d7ac618849d93081a20e1c0deccd2aef90901c9f5a9\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "# Libreria de busqueda de resultados google\n",
        "pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6d22c464",
      "metadata": {
        "id": "6d22c464"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ba25bb42",
      "metadata": {
        "id": "ba25bb42"
      },
      "outputs": [],
      "source": [
        "# Carga el modelo\n",
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0740ce8f",
      "metadata": {
        "id": "0740ce8f"
      },
      "outputs": [],
      "source": [
        "# Cargue algunas herramientas para usar\n",
        "\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"Ponga_su_API_aqui\"\n",
        "\n",
        "# Las herramientas serias equivalentes a los plugins de chatGPT\n",
        "# Guardamos herramientas para nuestro modelo \n",
        "# serpapi = WebScraper | llm-math = Ayuda para ecuaciones matematicas\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b20f48e3",
      "metadata": {
        "id": "b20f48e3"
      },
      "outputs": [],
      "source": [
        "# Finalmente, inicialicemos un agente con:\n",
        "# 1. Las herramientas\n",
        "# 2. El modelo de lenguaje\n",
        "# 3. El tipo de agente que queremos utilizar.\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f2cf54",
      "metadata": {
        "id": "45f2cf54"
      },
      "source": [
        "Ver lista de tipos de agentes [aquí](https://langchain.readthedocs.io/en/latest/modules/agents/agents.html?highlight=zero-shot-react-description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "89728919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "89728919",
        "outputId": "d419b8e8-4371-4912-94f5-a2d847ceb9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Japan is and then calculate the largest prime number that is smaller than their age.\n",
            "Action: Search\n",
            "Action Input: \"current leader of Japan\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of the leader of Japan\n",
            "Action: Search\n",
            "Action Input: \"age of Fumio Kishida\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m65 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than 65\n",
            "Action: Calculator\n",
            "Action Input: 65\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 65\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# ¡Ahora vamos a probarlo!\n",
        "agent.run(\"Who is the current leader of Japan? What is the largest prime number that is smaller than their age?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8808dc66",
      "metadata": {
        "id": "8808dc66"
      },
      "source": [
        "### Memory: Agregar estado a cadenas y agentes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "dc3294d3",
      "metadata": {
        "id": "dc3294d3"
      },
      "outputs": [],
      "source": [
        "# ConversationChain = Memory\n",
        "from langchain import OpenAI, ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "397ac43c",
      "metadata": {
        "id": "397ac43c"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Iniciamos la memoria del modelo\n",
        "conversation = ConversationChain(llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9823cde5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "9823cde5",
        "outputId": "8a254fe2-a81b-4e8a-95a0-0b9db7fcbf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "a3d547bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "a3d547bc",
        "outputId": "d575ac2a-aa4d-42d4-c220-ffe788d90413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "bb71a25c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "bb71a25c",
        "outputId": "eac9e4c8-2ac8-4902-d48a-a8056c7bd36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
            "Human: What was the first thing I said to you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You said \"Hi there!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "conversation.predict(input=\"What was the first thing I said to you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "0d7daf49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "0d7daf49",
        "outputId": "46f433e5-74f7-4fe6-e8b6-5bfa25aceb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
            "Human: What was the first thing I said to you?\n",
            "AI:  You said \"Hi there!\"\n",
            "Human: what is an alternative phrase for the first thing I said to you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' An alternative phrase for the first thing you said to me could be \"Greetings!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "conversation.predict(input=\"what is an alternative phrase for the first thing I said to you?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
